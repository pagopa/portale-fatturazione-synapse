{
	"name": "IngestInvoicing_timeline",
	"properties": {
		"folder": {
			"name": "Ingestion"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkcls01",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a068187c-c956-400f-8e7a-678aed46af1e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/5c587d75-fb91-4559-bda2-8612949a2b7d/resourceGroups/fat-d-analytics-rg/providers/Microsoft.Synapse/workspaces/fat-d-synw/bigDataPools/sparkcls01",
				"name": "sparkcls01",
				"type": "Spark",
				"endpoint": "https://fat-d-synw.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkcls01",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"#Variables\r\n",
					"tableNameinvoicing_timeline = 'fat_delta.invoicing_timeline'\r\n",
					"\r\n",
					"linked_service_name = 'nomeLinkedService'\r\n",
					"sourcefile = 'abfss://raw@fatddls.dfs.core.windows.net/send/2023/11/22/invoicing_timeline/invoicing_timeline_20231122.parquet'\r\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Import libraries"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import functions as F\r\n",
					"from delta.tables import *\r\n",
					"from pyspark.sql.functions import max\r\n",
					"from datetime import datetime"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Load and process source file"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"dfNewFile = spark.read.format(\"parquet\").load(sourcefile)\r\n",
					"actualTimeStamp = datetime.now()\r\n",
					"dfNewFile = dfNewFile.withColumn(\"insertionTimestamp\",  F.lit(actualTimeStamp))\r\n",
					"dfNewFile = dfNewFile.withColumn(\"SourceFile\",  F.lit(sourcefile))\r\n",
					"\r\n",
					""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"###TEST ON JOIN####\r\n",
					"\r\n",
					"#df= F.to_json(dfNewFile.select(F.col(\"details\")).collect())\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"#keys_df = dfNewFile.select(F.explode(F.map_keys(F.col(\"details\")))).distinct()\r\n",
					"# keys = list(map(lambda row: row[0], keys_df.collect()))\r\n",
					"# key_cols = list(map(lambda f: F.col(\"signals\").getItem(f).alias(str(f)), keys))\r\n",
					"# final_cols = [F.col(\"asset\"), F.col(\"ts\")] + key_cols\r\n",
					"# dfNewFile.select(final_cols).show()"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#spark.sql('DROP TABLE IF EXISTS fat_delta.notification')\r\n",
					"\r\n",
					"dfNewFile.write.format(\"delta\").mode(\"append\").saveAsTable(tableNameinvoicing_timeline)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Update watermark table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"watermarkTable =  DeltaTable.forName(spark, \"fat_delta.watermark\")\r\n",
					"\r\n",
					"\r\n",
					"watermarkTable.update(\"TableName = 'invoicing_timeline'\", {\"LastModified\": F.lit(actualTimeStamp)})\r\n",
					""
				],
				"execution_count": 5
			}
		]
	}
}